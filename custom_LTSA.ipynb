{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn code with comments highlighting our changes\n",
    "\"\"\"Locally Linear Embedding\"\"\"\n",
    "\n",
    "# Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>\n",
    "#         Jake Vanderplas  -- <vanderplas@astro.washington.edu>\n",
    "# License: BSD 3 clause (C) INRIA 2011\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh, svd, qr, solve\n",
    "from scipy.sparse import eye, csr_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "from sklearn.base import (\n",
    "    BaseEstimator,\n",
    "    TransformerMixin,\n",
    "    _UnstableArchMixin,\n",
    "    _ClassNamePrefixFeaturesOutMixin,\n",
    ")\n",
    "from sklearn.utils import check_random_state, check_array # change paths\n",
    "from sklearn.utils._arpack import _init_arpack_v0\n",
    "from sklearn.utils.extmath import stable_cumsum\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils.validation import FLOAT_DTYPES\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "def barycenter_weights(X, Y, indices, reg=1e-3):\n",
    "    \"\"\"Compute barycenter weights of X from Y along the first axis\n",
    "\n",
    "    We estimate the weights to assign to each point in Y[indices] to recover\n",
    "    the point X[i]. The barycenter weights sum to 1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_dim)\n",
    "\n",
    "    Y : array-like, shape (n_samples, n_dim)\n",
    "\n",
    "    indices : array-like, shape (n_samples, n_dim)\n",
    "            Indices of the points in Y used to compute the barycenter\n",
    "\n",
    "    reg : float, default=1e-3\n",
    "        amount of regularization to add for the problem to be\n",
    "        well-posed in the case of n_neighbors > n_dim\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    B : array-like, shape (n_samples, n_neighbors)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    See developers note for more information.\n",
    "    \"\"\"\n",
    "    X = check_array(X, dtype=FLOAT_DTYPES)\n",
    "    Y = check_array(Y, dtype=FLOAT_DTYPES)\n",
    "    indices = check_array(indices, dtype=int)\n",
    "\n",
    "    n_samples, n_neighbors = indices.shape\n",
    "    assert X.shape[0] == n_samples\n",
    "\n",
    "    B = np.empty((n_samples, n_neighbors), dtype=X.dtype)\n",
    "    v = np.ones(n_neighbors, dtype=X.dtype)\n",
    "\n",
    "    # this might raise a LinalgError if G is singular and has trace\n",
    "    # zero\n",
    "    for i, ind in enumerate(indices):\n",
    "        A = Y[ind]\n",
    "        C = A - X[i]  # broadcasting\n",
    "        G = np.dot(C, C.T)\n",
    "        trace = np.trace(G)\n",
    "        if trace > 0:\n",
    "            R = reg * trace\n",
    "        else:\n",
    "            R = reg\n",
    "        G.flat[:: n_neighbors + 1] += R\n",
    "        w = solve(G, v, sym_pos=True)\n",
    "        B[i, :] = w / np.sum(w)\n",
    "    return B\n",
    "\n",
    "\n",
    "def barycenter_kneighbors_graph(X, n_neighbors, reg=1e-3, n_jobs=None):\n",
    "    \"\"\"Computes the barycenter weighted graph of k-Neighbors for points in X\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : {array-like, NearestNeighbors}\n",
    "        Sample data, shape = (n_samples, n_features), in the form of a\n",
    "        numpy array or a NearestNeighbors object.\n",
    "\n",
    "    n_neighbors : int\n",
    "        Number of neighbors for each sample.\n",
    "\n",
    "    reg : float, default=1e-3\n",
    "        Amount of regularization when solving the least-squares\n",
    "        problem. Only relevant if mode='barycenter'. If None, use the\n",
    "        default.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        The number of parallel jobs to run for neighbors search.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : sparse matrix in CSR format, shape = [n_samples, n_samples]\n",
    "        A[i, j] is assigned the weight of edge that connects i to j.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    sklearn.neighbors.kneighbors_graph\n",
    "    sklearn.neighbors.radius_neighbors_graph\n",
    "    \"\"\"\n",
    "    knn = NearestNeighbors(n_neighbors=n_neighbors + 1, n_jobs=n_jobs).fit(X)\n",
    "    X = knn._fit_X\n",
    "    n_samples = knn.n_samples_fit_\n",
    "    ind = knn.kneighbors(X, return_distance=False)[:, 1:]\n",
    "    data = barycenter_weights(X, X, ind, reg=reg)\n",
    "    indptr = np.arange(0, n_samples * n_neighbors + 1, n_neighbors)\n",
    "    return csr_matrix((data.ravel(), ind.ravel(), indptr), shape=(n_samples, n_samples))\n",
    "\n",
    "\n",
    "def null_space(\n",
    "    M, k, k_skip=1, eigen_solver=\"arpack\", tol=1e-6, max_iter=100, random_state=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Find the null space of a matrix M.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    M : {array, matrix, sparse matrix, LinearOperator}\n",
    "        Input covariance matrix: should be symmetric positive semi-definite\n",
    "\n",
    "    k : int\n",
    "        Number of eigenvalues/vectors to return\n",
    "\n",
    "    k_skip : int, default=1\n",
    "        Number of low eigenvalues to skip.\n",
    "\n",
    "    eigen_solver : {'auto', 'arpack', 'dense'}, default='arpack'\n",
    "        auto : algorithm will attempt to choose the best method for input data\n",
    "        arpack : use arnoldi iteration in shift-invert mode.\n",
    "                    For this method, M may be a dense matrix, sparse matrix,\n",
    "                    or general linear operator.\n",
    "                    Warning: ARPACK can be unstable for some problems.  It is\n",
    "                    best to try several random seeds in order to check results.\n",
    "        dense  : use standard dense matrix operations for the eigenvalue\n",
    "                    decomposition.  For this method, M must be an array\n",
    "                    or matrix type.  This method should be avoided for\n",
    "                    large problems.\n",
    "\n",
    "    tol : float, default=1e-6\n",
    "        Tolerance for 'arpack' method.\n",
    "        Not used if eigen_solver=='dense'.\n",
    "\n",
    "    max_iter : int, default=100\n",
    "        Maximum number of iterations for 'arpack' method.\n",
    "        Not used if eigen_solver=='dense'\n",
    "\n",
    "    random_state : int, RandomState instance, default=None\n",
    "        Determines the random number generator when ``solver`` == 'arpack'.\n",
    "        Pass an int for reproducible results across multiple function calls.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "    \"\"\"\n",
    "    if eigen_solver == \"auto\":\n",
    "        if M.shape[0] > 200 and k + k_skip < 10:\n",
    "            eigen_solver = \"arpack\"\n",
    "        else:\n",
    "            eigen_solver = \"dense\"\n",
    "\n",
    "    if eigen_solver == \"arpack\":\n",
    "        v0 = _init_arpack_v0(M.shape[0], random_state)\n",
    "        try:\n",
    "            eigen_values, eigen_vectors = eigsh(\n",
    "                M, k + k_skip, sigma=0.0, tol=tol, maxiter=max_iter, v0=v0\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            raise ValueError(\n",
    "                \"Error in determining null-space with ARPACK. Error message: \"\n",
    "                \"'%s'. Note that eigen_solver='arpack' can fail when the \"\n",
    "                \"weight matrix is singular or otherwise ill-behaved. In that \"\n",
    "                \"case, eigen_solver='dense' is recommended. See online \"\n",
    "                \"documentation for more information.\" % e\n",
    "            ) from e\n",
    "\n",
    "        return eigen_vectors[:, k_skip:], np.sum(eigen_values[k_skip:])\n",
    "    elif eigen_solver == \"dense\":\n",
    "        if hasattr(M, \"toarray\"):\n",
    "            M = M.toarray()\n",
    "        eigen_values, eigen_vectors = eigh(\n",
    "            M, eigvals=(k_skip, k + k_skip - 1), overwrite_a=True\n",
    "        )\n",
    "        index = np.argsort(np.abs(eigen_values))\n",
    "        return eigen_vectors[:, index], np.sum(eigen_values)\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized eigen_solver '%s'\" % eigen_solver)\n",
    "\n",
    "\n",
    "def locally_linear_embedding(\n",
    "    X,\n",
    "    *,\n",
    "    n_neighbors,\n",
    "    n_components,\n",
    "    reg=1e-3,\n",
    "    eigen_solver=\"auto\",\n",
    "    tol=1e-6,\n",
    "    max_iter=100,\n",
    "    method=\"standard\",\n",
    "    hessian_tol=1e-4,\n",
    "    modified_tol=1e-12,\n",
    "    random_state=None,\n",
    "    n_jobs=None,\n",
    "):\n",
    "    \"\"\"Perform a Locally Linear Embedding analysis on the data.\n",
    "\n",
    "    Read more in the :ref:`User Guide <locally_linear_embedding>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : {array-like, NearestNeighbors}\n",
    "        Sample data, shape = (n_samples, n_features), in the form of a\n",
    "        numpy array or a NearestNeighbors object.\n",
    "\n",
    "    n_neighbors : int\n",
    "        number of neighbors to consider for each point.\n",
    "\n",
    "    n_components : int\n",
    "        number of coordinates for the manifold.\n",
    "\n",
    "    reg : float, default=1e-3\n",
    "        regularization constant, multiplies the trace of the local covariance\n",
    "        matrix of the distances.\n",
    "\n",
    "    eigen_solver : {'auto', 'arpack', 'dense'}, default='auto'\n",
    "        auto : algorithm will attempt to choose the best method for input data\n",
    "\n",
    "        arpack : use arnoldi iteration in shift-invert mode.\n",
    "                    For this method, M may be a dense matrix, sparse matrix,\n",
    "                    or general linear operator.\n",
    "                    Warning: ARPACK can be unstable for some problems.  It is\n",
    "                    best to try several random seeds in order to check results.\n",
    "\n",
    "        dense  : use standard dense matrix operations for the eigenvalue\n",
    "                    decomposition.  For this method, M must be an array\n",
    "                    or matrix type.  This method should be avoided for\n",
    "                    large problems.\n",
    "\n",
    "    tol : float, default=1e-6\n",
    "        Tolerance for 'arpack' method\n",
    "        Not used if eigen_solver=='dense'.\n",
    "\n",
    "    max_iter : int, default=100\n",
    "        maximum number of iterations for the arpack solver.\n",
    "\n",
    "    method : {'standard', 'hessian', 'modified', 'ltsa'}, default='standard'\n",
    "        standard : use the standard locally linear embedding algorithm.\n",
    "                   see reference [1]_\n",
    "        hessian  : use the Hessian eigenmap method.  This method requires\n",
    "                   n_neighbors > n_components * (1 + (n_components + 1) / 2.\n",
    "                   see reference [2]_\n",
    "        modified : use the modified locally linear embedding algorithm.\n",
    "                   see reference [3]_\n",
    "        ltsa     : use local tangent space alignment algorithm\n",
    "                   see reference [4]_\n",
    "\n",
    "    hessian_tol : float, default=1e-4\n",
    "        Tolerance for Hessian eigenmapping method.\n",
    "        Only used if method == 'hessian'\n",
    "\n",
    "    modified_tol : float, default=1e-12\n",
    "        Tolerance for modified LLE method.\n",
    "        Only used if method == 'modified'\n",
    "\n",
    "    random_state : int, RandomState instance, default=None\n",
    "        Determines the random number generator when ``solver`` == 'arpack'.\n",
    "        Pass an int for reproducible results across multiple function calls.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        The number of parallel jobs to run for neighbors search.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Y : array-like, shape [n_samples, n_components]\n",
    "        Embedding vectors.\n",
    "\n",
    "    squared_error : float\n",
    "        Reconstruction error for the embedding vectors. Equivalent to\n",
    "        ``norm(Y - W Y, 'fro')**2``, where W are the reconstruction weights.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    .. [1] Roweis, S. & Saul, L. Nonlinear dimensionality reduction\n",
    "        by locally linear embedding.  Science 290:2323 (2000).\n",
    "    .. [2] Donoho, D. & Grimes, C. Hessian eigenmaps: Locally\n",
    "        linear embedding techniques for high-dimensional data.\n",
    "        Proc Natl Acad Sci U S A.  100:5591 (2003).\n",
    "    .. [3] Zhang, Z. & Wang, J. MLLE: Modified Locally Linear\n",
    "        Embedding Using Multiple Weights.\n",
    "        http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382\n",
    "    .. [4] Zhang, Z. & Zha, H. Principal manifolds and nonlinear\n",
    "        dimensionality reduction via tangent space alignment.\n",
    "        Journal of Shanghai Univ.  8:406 (2004)\n",
    "    \"\"\"\n",
    "    if eigen_solver not in (\"auto\", \"arpack\", \"dense\"):\n",
    "        raise ValueError(\"unrecognized eigen_solver '%s'\" % eigen_solver)\n",
    "\n",
    "    if method not in (\"standard\", \"hessian\", \"modified\", \"ltsa\"):\n",
    "        raise ValueError(\"unrecognized method '%s'\" % method)\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors + 1, n_jobs=n_jobs, metric = geodesicDistance) # change metric\n",
    "    nbrs.fit(X)\n",
    "    X = nbrs._fit_X\n",
    "\n",
    "    N, d_in = X.shape\n",
    "\n",
    "    if n_components > d_in:\n",
    "        raise ValueError(\n",
    "            \"output dimension must be less than or equal to input dimension\"\n",
    "        )\n",
    "    if n_neighbors >= N:\n",
    "        raise ValueError(\n",
    "            \"Expected n_neighbors <= n_samples,  but n_samples = %d, n_neighbors = %d\"\n",
    "            % (N, n_neighbors)\n",
    "        )\n",
    "\n",
    "    if n_neighbors <= 0:\n",
    "        raise ValueError(\"n_neighbors must be positive\")\n",
    "\n",
    "    M_sparse = eigen_solver != \"dense\"\n",
    "\n",
    "    if method == \"standard\":\n",
    "        W = barycenter_kneighbors_graph(\n",
    "            nbrs, n_neighbors=n_neighbors, reg=reg, n_jobs=n_jobs\n",
    "        )\n",
    "\n",
    "        # we'll compute M = (I-W)'(I-W)\n",
    "        # depending on the solver, we'll do this differently\n",
    "        if M_sparse:\n",
    "            M = eye(*W.shape, format=W.format) - W\n",
    "            M = (M.T * M).tocsr()\n",
    "        else:\n",
    "            M = (W.T * W - W.T - W).toarray()\n",
    "            M.flat[:: M.shape[0] + 1] += 1  # W = W - I = W - I\n",
    "\n",
    "    elif method == \"hessian\":\n",
    "        dp = n_components * (n_components + 1) // 2\n",
    "\n",
    "        if n_neighbors <= n_components + dp:\n",
    "            raise ValueError(\n",
    "                \"for method='hessian', n_neighbors must be \"\n",
    "                \"greater than \"\n",
    "                \"[n_components * (n_components + 3) / 2]\"\n",
    "            )\n",
    "\n",
    "        neighbors = nbrs.kneighbors(\n",
    "            X, n_neighbors=n_neighbors + 1, return_distance=False,\n",
    "        )\n",
    "        neighbors = neighbors[:, 1:]\n",
    "\n",
    "        Yi = np.empty((n_neighbors, 1 + n_components + dp), dtype=np.float64)\n",
    "        Yi[:, 0] = 1\n",
    "\n",
    "        M = np.zeros((N, N), dtype=np.float64)\n",
    "\n",
    "        use_svd = n_neighbors > d_in\n",
    "\n",
    "        for i in range(N):\n",
    "            Gi = X[neighbors[i]]\n",
    "            Gi -= Gi.mean(0)\n",
    "\n",
    "            # build Hessian estimator\n",
    "            if use_svd:\n",
    "                U = svd(Gi, full_matrices=0)[0]\n",
    "            else:\n",
    "                Ci = np.dot(Gi, Gi.T)\n",
    "                U = eigh(Ci)[1][:, ::-1]\n",
    "\n",
    "            Yi[:, 1 : 1 + n_components] = U[:, :n_components]\n",
    "\n",
    "            j = 1 + n_components\n",
    "            for k in range(n_components):\n",
    "                Yi[:, j : j + n_components - k] = U[:, k : k + 1] * U[:, k:n_components]\n",
    "                j += n_components - k\n",
    "\n",
    "            Q, R = qr(Yi)\n",
    "\n",
    "            w = Q[:, n_components + 1 :]\n",
    "            S = w.sum(0)\n",
    "\n",
    "            S[np.where(abs(S) < hessian_tol)] = 1\n",
    "            w /= S\n",
    "\n",
    "            nbrs_x, nbrs_y = np.meshgrid(neighbors[i], neighbors[i])\n",
    "            M[nbrs_x, nbrs_y] += np.dot(w, w.T)\n",
    "\n",
    "        if M_sparse:\n",
    "            M = csr_matrix(M)\n",
    "\n",
    "    elif method == \"modified\":\n",
    "        if n_neighbors < n_components:\n",
    "            raise ValueError(\"modified LLE requires n_neighbors >= n_components\")\n",
    "\n",
    "        neighbors = nbrs.kneighbors(\n",
    "            X, n_neighbors=n_neighbors + 1, return_distance=False\n",
    "        )\n",
    "        neighbors = neighbors[:, 1:]\n",
    "\n",
    "        # find the eigenvectors and eigenvalues of each local covariance\n",
    "        # matrix. We want V[i] to be a [n_neighbors x n_neighbors] matrix,\n",
    "        # where the columns are eigenvectors\n",
    "        V = np.zeros((N, n_neighbors, n_neighbors))\n",
    "        nev = min(d_in, n_neighbors)\n",
    "        evals = np.zeros([N, nev])\n",
    "\n",
    "        # choose the most efficient way to find the eigenvectors\n",
    "        use_svd = n_neighbors > d_in\n",
    "\n",
    "        if use_svd:\n",
    "            for i in range(N):\n",
    "                X_nbrs = X[neighbors[i]] - X[i]\n",
    "                V[i], evals[i], _ = svd(X_nbrs, full_matrices=True)\n",
    "            evals **= 2\n",
    "        else:\n",
    "            for i in range(N):\n",
    "                X_nbrs = X[neighbors[i]] - X[i]\n",
    "                C_nbrs = np.dot(X_nbrs, X_nbrs.T)\n",
    "                evi, vi = eigh(C_nbrs)\n",
    "                evals[i] = evi[::-1]\n",
    "                V[i] = vi[:, ::-1]\n",
    "\n",
    "        # find regularized weights: this is like normal LLE.\n",
    "        # because we've already computed the SVD of each covariance matrix,\n",
    "        # it's faster to use this rather than np.linalg.solve\n",
    "        reg = 1e-3 * evals.sum(1)\n",
    "\n",
    "        tmp = np.dot(V.transpose(0, 2, 1), np.ones(n_neighbors))\n",
    "        tmp[:, :nev] /= evals + reg[:, None]\n",
    "        tmp[:, nev:] /= reg[:, None]\n",
    "\n",
    "        w_reg = np.zeros((N, n_neighbors))\n",
    "        for i in range(N):\n",
    "            w_reg[i] = np.dot(V[i], tmp[i])\n",
    "        w_reg /= w_reg.sum(1)[:, None]\n",
    "\n",
    "        # calculate eta: the median of the ratio of small to large eigenvalues\n",
    "        # across the points.  This is used to determine s_i, below\n",
    "        rho = evals[:, n_components:].sum(1) / evals[:, :n_components].sum(1)\n",
    "        eta = np.median(rho)\n",
    "\n",
    "        # find s_i, the size of the \"almost null space\" for each point:\n",
    "        # this is the size of the largest set of eigenvalues\n",
    "        # such that Sum[v; v in set]/Sum[v; v not in set] < eta\n",
    "        s_range = np.zeros(N, dtype=int)\n",
    "        evals_cumsum = stable_cumsum(evals, 1)\n",
    "        eta_range = evals_cumsum[:, -1:] / evals_cumsum[:, :-1] - 1\n",
    "        for i in range(N):\n",
    "            s_range[i] = np.searchsorted(eta_range[i, ::-1], eta)\n",
    "        s_range += n_neighbors - nev  # number of zero eigenvalues\n",
    "\n",
    "        # Now calculate M.\n",
    "        # This is the [N x N] matrix whose null space is the desired embedding\n",
    "        M = np.zeros((N, N), dtype=np.float64)\n",
    "        for i in range(N):\n",
    "            s_i = s_range[i]\n",
    "\n",
    "            # select bottom s_i eigenvectors and calculate alpha\n",
    "            Vi = V[i, :, n_neighbors - s_i :]\n",
    "            alpha_i = np.linalg.norm(Vi.sum(0)) / np.sqrt(s_i)\n",
    "\n",
    "            # compute Householder matrix which satisfies\n",
    "            #  Hi*Vi.T*ones(n_neighbors) = alpha_i*ones(s)\n",
    "            # using prescription from paper\n",
    "            h = np.full(s_i, alpha_i) - np.dot(Vi.T, np.ones(n_neighbors))\n",
    "\n",
    "            norm_h = np.linalg.norm(h)\n",
    "            if norm_h < modified_tol:\n",
    "                h *= 0\n",
    "            else:\n",
    "                h /= norm_h\n",
    "\n",
    "            # Householder matrix is\n",
    "            #  >> Hi = np.identity(s_i) - 2*np.outer(h,h)\n",
    "            # Then the weight matrix is\n",
    "            #  >> Wi = np.dot(Vi,Hi) + (1-alpha_i) * w_reg[i,:,None]\n",
    "            # We do this much more efficiently:\n",
    "            Wi = Vi - 2 * np.outer(np.dot(Vi, h), h) + (1 - alpha_i) * w_reg[i, :, None]\n",
    "\n",
    "            # Update M as follows:\n",
    "            # >> W_hat = np.zeros( (N,s_i) )\n",
    "            # >> W_hat[neighbors[i],:] = Wi\n",
    "            # >> W_hat[i] -= 1\n",
    "            # >> M += np.dot(W_hat,W_hat.T)\n",
    "            # We can do this much more efficiently:\n",
    "            nbrs_x, nbrs_y = np.meshgrid(neighbors[i], neighbors[i])\n",
    "            M[nbrs_x, nbrs_y] += np.dot(Wi, Wi.T)\n",
    "            Wi_sum1 = Wi.sum(1)\n",
    "            M[i, neighbors[i]] -= Wi_sum1\n",
    "            M[neighbors[i], i] -= Wi_sum1\n",
    "            M[i, i] += s_i\n",
    "\n",
    "        if M_sparse:\n",
    "            M = csr_matrix(M)\n",
    "\n",
    "    elif method == \"ltsa\":\n",
    "        neighbors = nbrs.kneighbors(\n",
    "            X, n_neighbors=n_neighbors + 1, return_distance=False,\n",
    "        )\n",
    "        neighbors = neighbors[:, 1:]\n",
    "\n",
    "        M = np.zeros((N, N))\n",
    "\n",
    "        use_svd = n_neighbors > d_in\n",
    "\n",
    "        for i in range(N):\n",
    "            Xi = X[neighbors[i]]\n",
    "            Xi -= Xi.mean(0)\n",
    "\n",
    "            # compute n_components largest eigenvalues of Xi * Xi^T\n",
    "            if use_svd:\n",
    "                v = svd(Xi, full_matrices=True)[0]\n",
    "            else:\n",
    "                Ci = np.dot(Xi, Xi.T)\n",
    "                v = eigh(Ci)[1][:, ::-1]\n",
    "\n",
    "            Gi = np.zeros((n_neighbors, n_components + 1))\n",
    "            Gi[:, 1:] = v[:, :n_components]\n",
    "            Gi[:, 0] = 1.0 / np.sqrt(n_neighbors)\n",
    "\n",
    "            GiGiT = np.dot(Gi, Gi.T)\n",
    "\n",
    "            nbrs_x, nbrs_y = np.meshgrid(neighbors[i], neighbors[i])\n",
    "            M[nbrs_x, nbrs_y] -= GiGiT\n",
    "            M[neighbors[i], neighbors[i]] += 1\n",
    "\n",
    "    return null_space(\n",
    "        M,\n",
    "        n_components,\n",
    "        k_skip=1,\n",
    "        eigen_solver=eigen_solver,\n",
    "        tol=tol,\n",
    "        max_iter=max_iter,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "\n",
    "class CustomLocallyLinearEmbedding(\n",
    "    _ClassNamePrefixFeaturesOutMixin,\n",
    "    TransformerMixin,\n",
    "    _UnstableArchMixin,\n",
    "    BaseEstimator,\n",
    "):\n",
    "    \"\"\"Locally Linear Embedding.\n",
    "\n",
    "    Read more in the :ref:`User Guide <locally_linear_embedding>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_neighbors : int, default=5\n",
    "        Number of neighbors to consider for each point.\n",
    "\n",
    "    n_components : int, default=2\n",
    "        Number of coordinates for the manifold.\n",
    "\n",
    "    reg : float, default=1e-3\n",
    "        Regularization constant, multiplies the trace of the local covariance\n",
    "        matrix of the distances.\n",
    "\n",
    "    eigen_solver : {'auto', 'arpack', 'dense'}, default='auto'\n",
    "        The solver used to compute the eigenvectors. The available options are:\n",
    "\n",
    "        - `'auto'` : algorithm will attempt to choose the best method for input\n",
    "          data.\n",
    "        - `'arpack'` : use arnoldi iteration in shift-invert mode. For this\n",
    "          method, M may be a dense matrix, sparse matrix, or general linear\n",
    "          operator.\n",
    "        - `'dense'`  : use standard dense matrix operations for the eigenvalue\n",
    "          decomposition. For this method, M must be an array or matrix type.\n",
    "          This method should be avoided for large problems.\n",
    "\n",
    "        .. warning::\n",
    "           ARPACK can be unstable for some problems.  It is best to try several\n",
    "           random seeds in order to check results.\n",
    "\n",
    "    tol : float, default=1e-6\n",
    "        Tolerance for 'arpack' method\n",
    "        Not used if eigen_solver=='dense'.\n",
    "\n",
    "    max_iter : int, default=100\n",
    "        Maximum number of iterations for the arpack solver.\n",
    "        Not used if eigen_solver=='dense'.\n",
    "\n",
    "    method : {'standard', 'hessian', 'modified', 'ltsa'}, default='standard'\n",
    "        - `standard`: use the standard locally linear embedding algorithm. see\n",
    "          reference [1]_\n",
    "        - `hessian`: use the Hessian eigenmap method. This method requires\n",
    "          ``n_neighbors > n_components * (1 + (n_components + 1) / 2``. see\n",
    "          reference [2]_\n",
    "        - `modified`: use the modified locally linear embedding algorithm.\n",
    "          see reference [3]_\n",
    "        - `ltsa`: use local tangent space alignment algorithm. see\n",
    "          reference [4]_\n",
    "\n",
    "    hessian_tol : float, default=1e-4\n",
    "        Tolerance for Hessian eigenmapping method.\n",
    "        Only used if ``method == 'hessian'``.\n",
    "\n",
    "    modified_tol : float, default=1e-12\n",
    "        Tolerance for modified LLE method.\n",
    "        Only used if ``method == 'modified'``.\n",
    "\n",
    "    neighbors_algorithm : {'auto', 'brute', 'kd_tree', 'ball_tree'}, \\\n",
    "                          default='auto'\n",
    "        Algorithm to use for nearest neighbors search, passed to\n",
    "        :class:`~sklearn.neighbors.NearestNeighbors` instance.\n",
    "\n",
    "    random_state : int, RandomState instance, default=None\n",
    "        Determines the random number generator when\n",
    "        ``eigen_solver`` == 'arpack'. Pass an int for reproducible results\n",
    "        across multiple function calls. See :term:`Glossary <random_state>`.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        The number of parallel jobs to run.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    embedding_ : array-like, shape [n_samples, n_components]\n",
    "        Stores the embedding vectors\n",
    "\n",
    "    reconstruction_error_ : float\n",
    "        Reconstruction error associated with `embedding_`\n",
    "\n",
    "    n_features_in_ : int\n",
    "        Number of features seen during :term:`fit`.\n",
    "\n",
    "        .. versionadded:: 0.24\n",
    "\n",
    "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "        Names of features seen during :term:`fit`. Defined only when `X`\n",
    "        has feature names that are all strings.\n",
    "\n",
    "        .. versionadded:: 1.0\n",
    "\n",
    "    nbrs_ : NearestNeighbors object\n",
    "        Stores nearest neighbors instance, including BallTree or KDtree\n",
    "        if applicable.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    SpectralEmbedding : Spectral embedding for non-linear dimensionality\n",
    "        reduction.\n",
    "    TSNE : Distributed Stochastic Neighbor Embedding.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    .. [1] Roweis, S. & Saul, L. Nonlinear dimensionality reduction\n",
    "        by locally linear embedding.  Science 290:2323 (2000).\n",
    "    .. [2] Donoho, D. & Grimes, C. Hessian eigenmaps: Locally\n",
    "        linear embedding techniques for high-dimensional data.\n",
    "        Proc Natl Acad Sci U S A.  100:5591 (2003).\n",
    "    .. [3] Zhang, Z. & Wang, J. MLLE: Modified Locally Linear\n",
    "        Embedding Using Multiple Weights.\n",
    "        http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382\n",
    "    .. [4] Zhang, Z. & Zha, H. Principal manifolds and nonlinear\n",
    "        dimensionality reduction via tangent space alignment.\n",
    "        Journal of Shanghai Univ.  8:406 (2004)\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_digits\n",
    "    >>> from sklearn.manifold import LocallyLinearEmbedding\n",
    "    >>> X, _ = load_digits(return_X_y=True)\n",
    "    >>> X.shape\n",
    "    (1797, 64)\n",
    "    >>> embedding = LocallyLinearEmbedding(n_components=2)\n",
    "    >>> X_transformed = embedding.fit_transform(X[:100])\n",
    "    >>> X_transformed.shape\n",
    "    (100, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        n_neighbors=5,\n",
    "        n_components=2,\n",
    "        reg=1e-3,\n",
    "        eigen_solver=\"auto\",\n",
    "        tol=1e-6,\n",
    "        max_iter=100,\n",
    "        method=\"standard\",\n",
    "        hessian_tol=1e-4,\n",
    "        modified_tol=1e-12,\n",
    "        neighbors_algorithm=\"auto\",\n",
    "        random_state=None,\n",
    "        n_jobs=None,\n",
    "    ):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.n_components = n_components\n",
    "        self.reg = reg\n",
    "        self.eigen_solver = eigen_solver\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.method = method\n",
    "        self.hessian_tol = hessian_tol\n",
    "        self.modified_tol = modified_tol\n",
    "        self.random_state = random_state\n",
    "        self.neighbors_algorithm = neighbors_algorithm\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def _fit_transform(self, X):\n",
    "        self.nbrs_ = NearestNeighbors(\n",
    "            n_neighbors=self.n_neighbors,\n",
    "            algorithm=self.neighbors_algorithm,\n",
    "            n_jobs=self.n_jobs, metric = geodesicDistance, # change metric\n",
    "        )\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        X = self._validate_data(X, dtype=float)\n",
    "        self.nbrs_.fit(X)\n",
    "        self.embedding_, self.reconstruction_error_ = locally_linear_embedding(\n",
    "            X=self.nbrs_,\n",
    "            n_neighbors=self.n_neighbors,\n",
    "            n_components=self.n_components,\n",
    "            eigen_solver=self.eigen_solver,\n",
    "            tol=self.tol,\n",
    "            max_iter=self.max_iter,\n",
    "            method=self.method,\n",
    "            hessian_tol=self.hessian_tol,\n",
    "            modified_tol=self.modified_tol,\n",
    "            random_state=random_state,\n",
    "            reg=self.reg,\n",
    "            n_jobs=self.n_jobs,\n",
    "        )\n",
    "        self._n_features_out = self.embedding_.shape[1]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Compute the embedding vectors for data X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training set.\n",
    "\n",
    "        y : Ignored\n",
    "            Not used, present here for API consistency by convention.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted `LocallyLinearEmbedding` class instance.\n",
    "        \"\"\"\n",
    "        self._fit_transform(X)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Compute the embedding vectors for data X and transform X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training set.\n",
    "\n",
    "        y : Ignored\n",
    "            Not used, present here for API consistency by convention.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like, shape (n_samples, n_components)\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "        self._fit_transform(X)\n",
    "        return self.embedding_\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform new points into embedding space.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : ndarray of shape (n_samples, n_components)\n",
    "            Returns the instance itself.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Because of scaling performed by this method, it is discouraged to use\n",
    "        it together with methods that are not scale-invariant (like SVMs).\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        X = self._validate_data(X, reset=False)\n",
    "        ind = self.nbrs_.kneighbors(\n",
    "            X, n_neighbors=self.n_neighbors, return_distance=False\n",
    "        )\n",
    "        weights = barycenter_weights(X, self.nbrs_._fit_X, ind, reg=self.reg)\n",
    "        X_new = np.empty((X.shape[0], self.n_components))\n",
    "        for i in range(X.shape[0]):\n",
    "            X_new[i] = np.dot(self.embedding_[ind[i]].T, weights[i])\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
